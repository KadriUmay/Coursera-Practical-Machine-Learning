library(xlsx)
install.packages('rJava', .libPaths()[1], 'http://www.rforge.net/')
library(xlsx)
install.packages("xlsx")
library(xlsx)
library(xlsx)
dat<-read.xlsx("ngap.xlsx",sheetIndex=1,header=TRUE,colIndex=colIndex,rowIndex=rowIndex)
library(xlsx)
library(xlsx)
path
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./pid.csv", method="curl")
library(data.table)
DT <- fread("./pid.csv")
file.info("./pid.csv")$size
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./pid.csv", method="curl")
library(data.table)
install.packages("data.table")
download.file(fileUrl, destfile="./pid.csv", method="curl")
library(data.table)
DT <- fread("./pid.csv")
file.info("./pid.csv")$size
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))+system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(rowMeans(DT)[DT$SEX==1])+system.time(rowMeans(DT)[DT$SEX==2])
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))+system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(rowMeans(DT)[DT$SEX==1])+system.time(rowMeans(DT)[DT$SEX==2])
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))+system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
library(xlsx)
library(xlsx)
colIndex <- 7:15
rowIndex <- 18:23
dat<-read.xlsx("ngap.xlsx",sheetIndex=1,header=TRUE,colIndex=colIndex,rowIndex=rowIndex)
install.packages("xlsx")
library(xlsx)
find.java <- function() {
for (root in c("HLM", "HCU")) for (key in c("Software\\JavaSoft\\Java Runtime Environment",
"Software\\JavaSoft\\Java Development Kit")) {
hive <- try(utils::readRegistry(key, root, 2),
silent = TRUE)
if (!inherits(hive, "try-error"))
return(hive)
}
hive
}
find.java()
library(xlsx)
library(xlsx)
library(xlsx)
library(xlsx)
library(xlsx)
install.packages("xlsx")
library(xlsx)
dat<-read.xlsx("ngap.xlsx",sheetIndex=1,header=TRUE,colIndex=colIndex,rowIndex=rowIndex)
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile="ngap.xlsx", method="curl", mode="wb")
dateDownloaded <- date()
colIndex <- 7:15
rowIndex <- 18:23
dat<-read.xlsx("ngap.xlsx",sheetIndex=1,header=TRUE,colIndex=colIndex,rowIndex=rowIndex)
dat
hist(x)
x<-rnorm(100)
hist(x)
par(oma)
par("oma")
par("mfrow")
y<-rnorm(100)
plot(x,y)
z<-rnorm(z)
x<-rnorm(100)
hist(x)
y<-rnorm(100)
plot(x,y)
z<-rnorm(z)
x<-rnorm(100)
hist(x)
y<-rnorm(100)
plot(x,y)
z<-rnorm(100)
par(mar=c(2,2,2,2))
plot(x,y)
par(mar=c(4,4,2,2))
plot(x,y)
plot(x,y, pch=19)
plot(x,y, pch=2)
plot(x,y, pch=1)
plot(x,y, pch=4)
plot(x,y, pch=8)
plot(x,y, pch=50)
example(points)
pchShow()
x<-rnorm(100)
hist(x)
y<-rnorm(100)
plot(x,y)
z<-rnorm(z)
plot(x,y, pch=20)
title("Scatter")
text(-2, -2, "Labale")
legend("topleft", legend="Data", pch=20)
fit<-lm(y~x)
abline(fit)
abline(fit, lwd=3)
abline(fit, lwd=3, col="blue")
plot(x,y, pch=20, xlab="weight", ylab="height")
?Devices
xyplot(weight ~ Time | Diet, BodyWeight)
install.packages(lattice)
install.packages("lattice")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(datasets)
data(airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
install.packages("ggplot2")
library(ggplot2)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies)+geom_smooth()
install.packages("httr")
library("httr")
oauth_endpoints("Week2Quiz")
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
stop_for_status(req)
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
9 stop_for_status(req)
10 content(req)
install.packages("httr")
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
stop_for_status(req)
content(req)
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
stop_for_status(req)
content(req)
oauth_endpoints("github")
myapp <- oauth_app("github", "7fd95f30b0d7039504cf", "acc7f06fd2d1a0e1b881635fe5f9f0ceb592761b")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/rate_limit", config(token = github_token))
stop_for_status(req)
content(req)
# curl -u Access Token:x-oauth-basic "https://api.github.com/users/jtleek/repos"
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
# 2013-11-07T13:25:07Z
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
file.dest <- 'ACS.csv'
download.file(file.url, file.dest, method='curl' )
ACS <- read.csv('ACS.csv')
file.url <- 'http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
file.dest <- 'ACS.csv'
download.file(file.url, file.dest, method='curl' )
ACS <- read.csv('ACS.csv')
head(ACS)
names(ACS)
summary(ACS)
str(ACS)
ACS$agricultureLogical <- ifelse(ACS$ACR==3 & ACS$AGS==6,TRUE,FALSE)
which(ACS$agricultureLogical)
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg'
file.dest <- 'jeff.jpg'
download.file(file.url, file.dest, mode='wb' )
library(jpeg)
picture <- readJPEG('jeff.jpg', native=TRUE)
file.url <- 'http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg'
file.dest <- 'jeff.jpg'
download.file(file.url, file.dest, mode='wb' )
library(jpeg)
picture <- readJPEG('jeff.jpg', native=TRUE)
install.packages("jpeg")
file.url <- 'http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg'
file.dest <- 'jeff.jpg'
download.file(file.url, file.dest, mode='wb' )
library(jpeg)
picture <- readJPEG('jeff.jpg', native=TRUE)
head(package)
head(picture)
quantile(picture, probs = c(0.3, 0.8) )
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
file.dest <- 'GDP.csv'
download.file(file.url, file.dest )
rowNames <- seq(10,200, 2)
gdp <- read.csv('GDP.csv', header=F, skip=5, nrows=190)
View(gdp)
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv'
file.dest <- 'GDP2.csv'
download.file(file.url, file.dest )
fed <- read.csv('GDP2.csv')
View(fed)
View(gdp)
View(fed)
file.dest <- 'GDP.csv'
combined <- merge(gdp, fed, by.x='V1', by.y='CountryCode', sort=TRUE)
View(combined)
combined[with(combined, order(-V2) )]
combined[with(combined, order(-V2) )]
View(combined)
combined[with(combined, order(-V2))]
ordered<-order(combined, -V2)
ordered<-order(combined, -combined$V2)
combined[with(combined, order(-combined$V2))]
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
file.dest <- 'GDP.csv'
download.file(file.url, file.dest )
# specify the right lines
rowNames <- seq(10,200, 2)
gdp <- read.csv('GDP.csv', header=F, skip=5, nrows=190)
#View(gdp)
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv'
file.dest <- 'GDP2.csv'
download.file(file.url, file.dest )
fed <- read.csv('GDP2.csv')
#View(fed)
combined <- merge(gdp, fed, by.x='V1', by.y='CountryCode', sort=TRUE)
#View(combined)
head(combined$V2)
ordered<-arrange(combined, desc(V2))
library("plyr")
ordered<-arrange(combined, desc(V2))
View(ordered)
mean(combined[combined$Income.Group=='High income: OECD',]$V2)
mean(combined[combined$Income.Group=='High income: nonOECD',]$V2)
q1 <- combined$V2 <= 38
View(q1)
quentile <- c(0.2,0.4,0.6,0.8,1)
q <- quantile(combined$V2, quentile)
q1 <- combined$V2 <= 38
xtabs(q1 ~ combined$Income.Group)
View(ordered)
combined[with(combined, order(-V2))]
View(ordered)
count(ordered)
summary(ordered)
str(ordered)
ordered[12,]
ordered[ordered$V1==13]
ordered[,ordered$V1==13]
ordered[ordered$V1==13,]
ordered[13,]
getwd()
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
features <- read.table("./data/features.txt")
head(features)
install.packages("R.utils")
set.seed(3)
lambda <- 0.2
num_sim <- 1000
sample_size <- 40
sim <- matrix(rexp(num_sim*sample_size, rate=lambda), num_sim, sample_size)
head(sim)
dim(sim)
row_means <- rowMeans(sim)
head(row_means)
row_means
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
# density of the averages of samples
lines(density(row_means))
# theoretical center of distribution
abline(v=1/lambda, col="red")
# theoretical density of the averages of samples
xfit <- seq(min(row_means), max(row_means), length=100)
yfit <- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(sample_size)))
lines(xfit, yfit, pch=22, col="red", lty=2)
# add legend
legend('topright', c("simulation", "theoretical"), lty=c(1,2), col=c("black", "red"))
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
set.seed(3)
lambda <- 0.2
num_sim <- 1000
sample_size <- 40
sim <- matrix(rexp(num_sim*sample_size, rate=lambda), num_sim, sample_size)
head(sim)
dim(sim)
row_means <- rowMeans(sim)
head(row_means)
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
# density of the averages of samples
lines(density(row_means))
# theoretical center of distribution
abline(v=1/lambda, col="red")
# theoretical density of the averages of samples
xfit <- seq(min(row_means), max(row_means), length=100)
yfit <- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(sample_size)))
lines(xfit, yfit, pch=22, col="red", lty=2)
# add legend
legend('topright', c("simulation", "theoretical"), lty=c(1,2), col=c("black", "red"))
set.seed(3)
lambda <- 0.2
num_sim <- 1000
sample_size <- 40
sim <- matrix(rexp(num_sim*sample_size, rate=lambda), num_sim, sample_size)
head(sim)
dim(sim)
row_means <- rowMeans(sim)
head(row_means)
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
# density of the averages of samples
lines(density(row_means))
# theoretical center of distribution
abline(v=1/lambda, col="red")
# theoretical density of the averages of samples
xfit <- seq(min(row_means), max(row_means), length=100)
yfit <- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(sample_size)))
lines(xfit, yfit, pch=22, col="red", lty=2)
# add legend
legend('topright', c("simulation", "theoretical"), lty=c(1,2), col=c("black", "red"))
par(mfcol=c(12,12), oma=c(1,1,0,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
par(mfcol=c(12,12), oma=c(1,1,0,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
install.packages(methods)
library(methods)
?Classes
library(googleVis)
install.packages("googleVis")
ftype(show)
library(pryr)
intall.packages()
install.packages(pryr)
install.packages("pryr")
install.packages("RGtk2")
library(RGtk2)
install.packages("ElemStatLearn")
install.packages("e1071")
library(e1071)
remove.packages(e1071)
remove.packages("e1071")
install.packages(c("curl", "devtools", "digest", "httr", "kernlab", "knitr", "memoise", "mgcv", "nlme", "nnet", "pbkrtest", "R6", "rattle", "Rcpp", "RcppArmadillo", "RcppEigen", "rJava", "rmarkdown", "rstudioapi", "shiny"))
# R Script
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
#Set Directories
ProjectDir <- "c:\\users\\kadriu\\documents\\GitHub\\Coursera-Practical-Machine-Learning"
SubDir <- "Data"
setwd(ProjectDir)
if (!file.exists(SubDir)) {
dir.create(file.path(SubDir))
}
setwd(file.path(ProjectDir, SubDir))
#Download the Data
TrainLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TrainFile <- "pml-training.csv"
TestFile <- "pml-testing.csv"
if (!file.exists(TrainFile))
download.file(TrainLink, destfile = TrainFile, method = "curl")
if (!file.exists(TestFile))
download.file(TestLink, destfile = TestFile, method = "curl")
#Read Data and Load into Data Frames
RawTrainData <- read.csv(TrainFile)
RawTestData <- read.csv(TestFile)
#Raw Training Data Summary
#The 'classe' variable in the dataset is the variable to predict
#Number of Variables
dim(RawTrainData) #19622 160
str(RawTrainData)
#Raw Test Data Summary
#Number of Rows
dim(RawTestData) #20 160
str(RawTestData)
#check the rows of data which has complete cases
#Training Dataset
sum(complete.cases(RawTrainData)) #406 Very small part of the training data has complete data
#Test Dataset
sum(complete.cases(RawTestData)) #0 None of the test data has complete data
#Data Cleaning and Preperation
#We will remove the NAs and irrelevant variables
#In the training set check columns with total NA values greater then 10% of the rows (more than 2000 NAs)
RawTrainDataNZero <- RawTrainData[, colSums(is.na(RawTrainData)) < 2000]
ncol(RawTrainDataNZero) #93
#93 variables have total number of NAs greater then 10% of the total rows of data
#Check the numbers of columns which has one or more NAs
RawTrainDataZero <- RawTrainData[, colSums(is.na(RawTrainData)) == 0]
ncol(RawTrainDataZero) #93
#Again 93 columns have no NAs, no need for further detailed processing such as imputing
#Just remove the columns with NAs
RawTrainData <- RawTrainData[, colSums(is.na(RawTrainData)) == 0]
#In the testing set remove columns with one or more NA in the training dataset
RawTestData <- RawTestData[, colSums(is.na(RawTestData)) == 0]
#Further to this, we need to remove the unnecessary columns that do not contribute to the results
#These are
# $ X                      : int  1 2 3 4 5 6 7 8 9 10 ...
# $user_name:Factor w / 6 levels "adelmo", "carlitos", ..:2 2 2 2 2 2 2 2 2 2 ...
# $raw_timestamp_part_1:int 1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
# $raw_timestamp_part_2:int 788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...
# $cvtd_timestamp:Factor w / 20 levels "02/12/2011 13:32", ..:9 9 9 9 9 9 9 9 9 9 ...
# $new_window:Factor w / 2 levels "no", "yes":1 1 1 1 1 1 1 1 1 1 ...
#$ num_window:int 11 11 11 12 12 12 12 12 12 12 ...
TrainColsToRemove <- grepl("^X|user_name|timestamp|window", names(RawTrainData))
RawTrainData <- RawTrainData[, !TrainColsToRemove]
TestColsToRemove <- grepl("^X|user_name|timestamp|window", names(RawTestData))
RawTestData <- RawTestData[, !TestColsToRemove]
#check the rows of data which has complete cases again
#Training Dataset
sum(complete.cases(RawTrainData)) #19622 We do now have complete cases for all training data
#Test Dataset
sum(complete.cases(RawTestData)) #20 We do now have complete cases for all test data
set.seed(562389) #for reproducible results
#We further the partition the training dataset for training and validataion purposes.
#We would like to the validate the model with a subset of the data before applying the test data
#This is to avoid overfitting
#Generate a training and validation dataset
TrainIdx <- createDataPartition(RawTrainData$classe, p = 0.7, list = FALSE)
TrainData <- RawTrainData[TrainIdx,]
TestData <- RawTrainData[ - TrainIdx,]
#Data Modelling
#Use Random Forests as it is the most robust algorithm and automatically selects the important variables
#In our case we don't have extensive insight on the data we have and we are exploring if we can predict
#Based on the data we have
#Also we don't the outcome to be human interpretable in this case, this is an early phase of research and
#there are no severe risks such as medical diagnosis and personlized medicine
#We do apply a 10-Fold cross validation to avoid overfitting
Control <- trainControl(method="cv", number = 10)
Model <- train(classe~., data = TrainData, method = "rf", ntree = "250", trControl = "Control")
Predict <- predict(Model, TestData)
confusionMatrix(TestData$classe, Predict)
str(TrainData)
#Remove the columns with NA
#Remove the first 5 columns as they're irrelevant
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
install.packages("Rcpp")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(caret)
install.packages("ggplot2")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(caret)
library(caret)
library(Rcpp)
library(caret)
