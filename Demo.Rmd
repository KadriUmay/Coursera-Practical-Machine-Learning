---
title: "Practical Machine Learning Course Report"
author: "Kadri Umay"
date: "February 9, 2016"
output: html_document
---

## Introduction 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

The approach proposed for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation  provides useful information for a large variety of applications,such as sports training.

The quality of execution is defined for Unilateral Dumbbell Biceps Curl in five different fashions. Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.
	* A - Exactly according to the specification 
	* B - Throwing the elbows to the front
	* C - Lifting the dumbbell only halfway
	* D - Lowering the dumbbell only halfway
	* E - Throwing the hips to the front

We will try to classify the quality of exeuction in one of the 5 different classes based on the data collected from the on-body sensors while the exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience and simulating the mistakes.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3z1Cxz0oe

## Data Preprocessing
```{r, cache = T}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(nnet)
library(e1071)
library(MASS)

```

### Set Directories and Download Data
Download Data and store in data frames
```{r, cache = T}
ProjectDir <- "c:\\users\\kadriu\\documents\\GitHub\\Coursera-Practical-Machine-Learning"
SubDir <- "Data"
setwd(ProjectDir)

if (!file.exists(SubDir)) {
  dir.create(file.path(SubDir))
}
setwd(file.path(ProjectDir, SubDir))

TrainLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TrainFile <- "pml-training.csv"
TestFile <- "pml-testing.csv"
if (!file.exists(TrainFile))
  download.file(TrainLink, destfile = TrainFile, method = "curl")
if (!file.exists(TestFile))
  download.file(TestLink, destfile = TestFile, method = "curl")

RawTrainData <- read.csv(TrainFile)
RawTestData <- read.csv(TestFile)
```

Raw Training Data Summary
The 'classe' variable in the dataset is the variable to predict

Number of Variables
```{r, cache = T}
dim(RawTrainData) #19622 160
str(RawTrainData, list.len=ncol(RawTrainData))
```

Raw Test Data Summary
Number of Rows
```{r, cache = T}
dim(RawTestData) #20 160
str(RawTestData, list.len=ncol(RawTestData))
```

Check the rows of data which has complete cases
Training Dataset
```{r, cache = T}
sum(complete.cases(RawTrainData)) #406 Very small part of the training data has complete data
```

Test Dataset
```{r, cache = T}
sum(complete.cases(RawTestData)) #0 None of the test data has complete data
```

### Data Cleaning and Preperation
We will remove the NAs and irrelevant variables

In the training set check columns with total NA values greater then 10% of the rows (more than 2000 NAs)
```{r, cache = T}
RawTrainDataNZero <- RawTrainData[, colSums(is.na(RawTrainData)) < 2000]
ncol(RawTrainDataNZero) #93
```

93 variables have total number of NAs greater then 10% of the total rows of data

Check the numbers of columns which has one or more NAs
```{r, cache = T}
RawTrainDataZero <- RawTrainData[, colSums(is.na(RawTrainData)) == 0]
ncol(RawTrainDataZero) #93
```

Again 93 columns have no NAs, no need for further detailed processing such as imputing
Just remove the columns with NAs
```{r, cache = T}
RawTrainData <- RawTrainData[, colSums(is.na(RawTrainData)) == 0]
```

In the testing set remove columns with one or more NA in the training dataset
```{r, cache = T}
RawTestData <- RawTestData[, colSums(is.na(RawTestData)) == 0]
```

Further to this, we need to remove the unnecessary columns that do not contribute to the results

These are:
  $X:int
  $user_name:Factor w / 6 levels
  $raw_timestamp_part_1:int
  $raw_timestamp_part_2:int
  $cvtd_timestamp:Factor w / 20 levels
  $new_window:Factor w / 2 levels
  $num_window:int

```{r, cache = T}
classe <- RawTrainData$classe
TrainColsToRemove <- grepl("^X|user_name|timestamp|window", names(RawTrainData))
RawTrainData <- RawTrainData[, !TrainColsToRemove]
CleanTrainData <- RawTrainData[, sapply(RawTrainData, is.numeric)]
CleanTrainData$classe <- classe

classe <- RawTestData$classe
TestColsToRemove <- grepl("^X|user_name|timestamp|window", names(RawTestData))
RawTestData <- RawTestData[, !TestColsToRemove]
CleanTestData <- RawTestData[, sapply(RawTestData, is.numeric)]
CleanTestData$classe <- classe
```

Check the rows of data which has complete cases again
Training Dataset
```{r, cache = T}
sum(complete.cases(RawTrainData)) #19622 We do now have complete cases for all training data
```
Test Dataset
```{r, cache = T}
sum(complete.cases(RawTestData)) #20 We do now have complete cases for all test data
```
Set seed for reproducible results
```{r, cache = T}
set.seed(562389)
```
We further the partition the training dataset for training and validataion purposes.
We would like to the validate the model with a subset of the data before applying the test data
This is to avoid overfitting
Generate a training and validation dataset
```{r, cache = T}
TrainIdx <- createDataPartition(CleanTrainData$classe, p = 0.7, list = FALSE)
TrainData <- CleanTrainData[TrainIdx,]
TestData <- CleanTrainData[ - TrainIdx,]
```

## Data Modelling
We will test several algorithms and compare their accuracy levels

### First one is the Random Forest
```{r, cache = T}
modelRf <- randomForest(classe~., data = TrainData)
predRf<-predict(modelRf,TestData)
confusionMatrix(TestData$classe, predRf)
```
### Second we test Artificial Neural Network
A simple single layer network with no hidden perceptrons
```{r, cache = T}
modelNn <- nnet(classe~., data = TrainData, size=15)
predNn<-predict(modelNn,TestData, TYPE="class")
```
Output of predictors for Artifical Neural Networks is different
Rather then giving the highhest probability prediction
It gives the probability for each prediction
We select the one with the highest
```{r, cache = T}
prdNN<-c("A","B","C","D","E")[apply(predNn,1,which.max)]
confusionMatrix(TestData$classe, prdNN)
```
### Third we test support vector algorithm
```{r, cache = T}
modelSvm <- svm(classe~., data = TrainData)
predSvm<-predict(modelSvm,TestData)
confusionMatrix(TestData$classe, predSvm)
```

### Fourth we would like to test Linear Discriminant Analysis for its simplicity
```{r, cache = T}
modelLda <- lda(classe~., data = TrainData)
predLda<-predict(modelLda,TestData)
confusionMatrix(TestData$classe, predLda$class)
```
Note that the output of predictor is also different in this case, we need to select
the class variable

### Fifth and last one is the regression tree
Which are useful for their human interpretable results
```{r, cache = T}
modelCart <- rpart(classe~., data = TrainData)
predCart<-predict(modelCart,TestData)
```
Output of predictors for Classification and Regression Tree is different
Rather then giving the highhest probability prediction
It gives the probability for each prediction
We select the one with the highest
```{r, cache = T}
prdCART<-c("A","B","C","D","E")[apply(predCart,1,which.max)]
confusionMatrix(TestData$classe, prdCART)
```
## Results of the comparision of accuracy of the different algorithms
```{r, cache = T}
OutputTable <- c("Random Forest",as.numeric(confusionMatrix(TestData$classe, predRf)$overall["Accuracy"]))
temp<- rbind(OutputTable, c("Artificial Neural Network",as.numeric(confusionMatrix(TestData$classe, prdNN)$overall["Accuracy"])))
temp<- rbind(temp, c("Support Vector",as.numeric(confusionMatrix(TestData$classe, predSvm)$overall["Accuracy"])))
temp<- rbind(temp, c("Linear Discriminant Analysis",as.numeric(confusionMatrix(TestData$classe, predLda$class)$overall["Accuracy"])))
temp<- rbind(temp, c("Classification and Regression Tree",as.numeric(confusionMatrix(TestData$classe, prdCART)$overall["Accuracy"])))
OutputTable <- temp
colnames(OutputTable) <- c("Algorithm", "Accuracy")
OutputTable
```
By far random forest is the most acurate one
We will use this algorithm for predicting the outcome for the course project 
```{r, cache = T}
predRf<-predict(modelRf,CleanTestData)
predRf
```
##Visual Analysis of the Model

An additional tree visualization would reveal the
Decision critieria
Especialy important to be analyzed by a domain expert
Sometime data driven models could bring some issues
```{r, cache = T}
treeModel <- rpart(classe ~ ., data=TrainData, method="class")
prp(treeModel)
```
Correlation Plot between different attributes of the model
The graph is not readable but gives an idea on the 
importance of some of the attributes of the model
```{r, cache = T}
corrPlot <- cor(TrainData[, -length(names(TrainData))])
corrplot(corrPlot, method="color")
```